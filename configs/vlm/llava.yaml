vlm_kwargs:
  min_vram_gb: 16.7
  vlm_id: llava-hf/llava-1.5-7b-hf
  vlm_cls: AutoModelForVision2Seq
  load_in_4bit: False
  torch_dtype: torch.bfloat16
vlm_template:
  image_token: "<image>"
  system_prompt: "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions."
  prefix: "${vlm_template.system_prompt} USER: {image_token}\n{question} ASSISTANT: "